{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/michael/anaconda3/envs/tensorlow-gpu/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/michael/anaconda3/envs/tensorlow-gpu/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/michael/anaconda3/envs/tensorlow-gpu/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/michael/anaconda3/envs/tensorlow-gpu/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/michael/anaconda3/envs/tensorlow-gpu/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/michael/anaconda3/envs/tensorlow-gpu/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras import layers, models, optimizers\n",
    "from keras import backend as K\n",
    "from keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import combine_images\n",
    "from PIL import Image\n",
    "from capsulelayers import CapsuleLayer, PrimaryCap, Length, Mask\n",
    "\n",
    "#importint the necessary lirbraries\n",
    "\n",
    "#import tensorflow.keras as kerasfrom __future__ import print_function\n",
    "\n",
    "import keras\n",
    "from keras.layers import Convolution2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras import optimizers\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.layers import Layer\n",
    "from keras import activations\n",
    "from keras import utils\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras import optimizers\n",
    "from keras.layers import *\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from os import listdir\n",
    "import os\n",
    "from imutils import paths\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import random\n",
    "\n",
    "K.set_image_data_format('channels_last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CapsNet(input_shape, n_class, routings):\n",
    "    \"\"\"\n",
    "    A Capsule Network on MNIST.\n",
    "    :param input_shape: data shape, 3d, [width, height, channels]\n",
    "    :param n_class: number of classes\n",
    "    :param routings: number of routing iterations\n",
    "    :return: Two Keras Models, the first one used for training, and the second one for evaluation.\n",
    "            `eval_model` can also be used for training.\n",
    "    \"\"\"\n",
    "    x = layers.Input(shape=input_shape)\n",
    "\n",
    "    # Layer 1: Just a conventional Conv2D layer\n",
    "    conv1 = layers.Conv2D(filters=256, kernel_size=9, strides=1, padding='valid', activation='relu', name='conv1')(x)\n",
    "\n",
    "    # Layer 2: Conv2D layer with `squash` activation, then reshape to [None, num_capsule, dim_capsule]\n",
    "    primarycaps = PrimaryCap(conv1, dim_capsule=8, n_channels=32, kernel_size=9, strides=2, padding='valid')\n",
    "\n",
    "    # Layer 3: Capsule layer. Routing algorithm works here.\n",
    "    digitcaps = CapsuleLayer(num_capsule=n_class, dim_capsule=16, routings=routings,\n",
    "                             name='digitcaps')(primarycaps)\n",
    "\n",
    "    # Layer 4: This is an auxiliary layer to replace each capsule with its length. Just to match the true label's shape.\n",
    "    # If using tensorflow, this will not be necessary. :)\n",
    "    out_caps = Length(name='capsnet')(digitcaps)\n",
    "\n",
    "    # Decoder network.\n",
    "    y = layers.Input(shape=(n_class,))\n",
    "    masked_by_y = Mask()([digitcaps, y])  # The true label is used to mask the output of capsule layer. For training\n",
    "    masked = Mask()(digitcaps)  # Mask using the capsule with maximal length. For prediction\n",
    "\n",
    "    # Shared Decoder model in training and prediction\n",
    "    decoder = models.Sequential(name='decoder')\n",
    "    decoder.add(layers.Dense(512, activation='relu', input_dim=16*n_class))\n",
    "    decoder.add(layers.Dense(1024, activation='relu'))\n",
    "    decoder.add(layers.Dense(np.prod(input_shape), activation='sigmoid'))\n",
    "    decoder.add(layers.Reshape(target_shape=input_shape, name='out_recon'))\n",
    "\n",
    "    # Models for training and evaluation (prediction)\n",
    "    train_model = models.Model([x, y], [out_caps, decoder(masked_by_y)])\n",
    "    eval_model = models.Model(x, [out_caps, decoder(masked)])\n",
    "\n",
    "    # manipulate model\n",
    "    noise = layers.Input(shape=(n_class, 16))\n",
    "    noised_digitcaps = layers.Add()([digitcaps, noise])\n",
    "    masked_noised_y = Mask()([noised_digitcaps, y])\n",
    "    manipulate_model = models.Model([x, y, noise], decoder(masked_noised_y))\n",
    "    return train_model, eval_model, manipulate_model\n",
    "\n",
    "\n",
    "def margin_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Margin loss for Eq.(4). When y_true[i, :] contains not just one `1`, this loss should work too. Not test it.\n",
    "    :param y_true: [None, n_classes]\n",
    "    :param y_pred: [None, num_capsule]\n",
    "    :return: a scalar loss value.\n",
    "    \"\"\"\n",
    "    L = y_true * K.square(K.maximum(0., 0.9 - y_pred)) + \\\n",
    "        0.5 * (1 - y_true) * K.square(K.maximum(0., y_pred - 0.1))\n",
    "\n",
    "    return K.mean(K.sum(L, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/michael/anaconda3/envs/tensorlow-gpu/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/michael/anaconda3/envs/tensorlow-gpu/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/michael/anaconda3/envs/tensorlow-gpu/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/michael/Desktop/Datascience Projects/goz-plant-disease-project/project/code/capsulelayers.py:145: calling softmax (from tensorflow.python.ops.nn_ops) with dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "dim is deprecated, use axis instead\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 28, 28, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 20, 20, 256)  20992       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_conv2d (Conv2D)      (None, 6, 6, 256)    5308672     conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_reshape (Reshape)    (None, 1152, 8)      0           primarycap_conv2d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_squash (Lambda)      (None, 1152, 8)      0           primarycap_reshape[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "digitcaps (CapsuleLayer)        (None, 10, 16)       1474560     primarycap_squash[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 10)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "mask_1 (Mask)                   (None, 160)          0           digitcaps[0][0]                  \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "capsnet (Length)                (None, 10)           0           digitcaps[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "decoder (Sequential)            (None, 28, 28, 1)    1411344     mask_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 8,215,568\n",
      "Trainable params: 8,215,568\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#define the model\n",
    "model, eval_model, manipulate_model = CapsNet(input_shape=(28, 28, 1),\n",
    "                                                  n_class=10,\n",
    "                                                  routings=3)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] splitting data\n"
     ]
    }
   ],
   "source": [
    "#Now rewriting the algorithm to preprocess about 500 images in my own pipeline\n",
    "#only do this if you have enough RAM\n",
    "def convert_image_to_array(image_dir):\n",
    "    try:\n",
    "        image = cv2.imread(image_dir)\n",
    "        if image is not None:\n",
    "            image = cv2.resize(image, (28, 28))\n",
    "            return img_to_array(image)\n",
    "        else:\n",
    "            return np.array([])\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error : {e}\")\n",
    "        return None\n",
    "\n",
    "image_list = []\n",
    "\n",
    "directory_root = '../dataset/greyscaled/'\n",
    "imagePaths = list(paths.list_images(directory_root))\n",
    "imagePaths = imagePaths[0:-1:2]\n",
    "random.shuffle(imagePaths)\n",
    "labels = [p.split(os.path.sep)[-2] for p in imagePaths]\n",
    "\n",
    "batch_size = 16\n",
    "try:\n",
    "    for i in np.arange(0, len(imagePaths), batch_size):\n",
    "        batchPaths = imagePaths[i:i + batch_size]\n",
    "        batchLabels = labels[i: i + batch_size]\n",
    "        \n",
    "        for image in batchPaths:\n",
    "            image_list.append(convert_image_to_array(image))\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "\n",
    "len(image_list), len(labels)\n",
    "\n",
    "np_image_list = np.array(image_list, dtype = np.float16) / 255.0\n",
    "print(f\"[INFO] splitting data\")\n",
    "x_train, x_test, y_train, y_test = train_test_split(np_image_list, labels, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9080, 28, 28, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_image_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = LabelBinarizer().fit_transform(y_test)\n",
    "y_train = LabelBinarizer().fit_transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7264, 28, 28, 3), (1816, 28, 28, 3))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = np.array(x_train)\n",
    "x_test = np.array(x_test)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "x_train.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7264, 28, 28, 3), (1816, 28, 28, 3))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = x_train.reshape((x_train.shape[0], 28, 28, 3))\n",
    "x_test = x_test.reshape((x_test.shape[0], 28, 28, 3))\n",
    "x_train.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1816, 10)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape, len(y_test)\n",
    "classNames = [str(x) for x in np.unique(labels)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/michael/anaconda3/envs/tensorlow-gpu/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/michael/anaconda3/envs/tensorlow-gpu/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/michael/anaconda3/envs/tensorlow-gpu/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/michael/anaconda3/envs/tensorlow-gpu/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "Train on 7264 samples, validate on 1816 samples\n",
      "Epoch 1/100\n",
      "7264/7264 [==============================] - 216s 30ms/step - loss: 0.4217 - capsnet_loss: 0.4120 - decoder_loss: 0.0247 - capsnet_acc: 0.4200 - val_loss: 0.3313 - val_capsnet_loss: 0.3223 - val_decoder_loss: 0.0231 - val_capsnet_acc: 0.5749\n",
      "\n",
      "Epoch 00001: val_capsnet_acc improved from -inf to 0.57489, saving model to ../models/best_weights_capsule_new_train.h5\n",
      "Epoch 2/100\n",
      "7264/7264 [==============================] - 199s 27ms/step - loss: 0.3043 - capsnet_loss: 0.2957 - decoder_loss: 0.0220 - capsnet_acc: 0.5976 - val_loss: 0.2770 - val_capsnet_loss: 0.2684 - val_decoder_loss: 0.0219 - val_capsnet_acc: 0.6564\n",
      "\n",
      "Epoch 00002: val_capsnet_acc improved from 0.57489 to 0.65639, saving model to ../models/best_weights_capsule_new_train.h5\n",
      "Epoch 3/100\n",
      "7264/7264 [==============================] - 190s 26ms/step - loss: 0.2660 - capsnet_loss: 0.2577 - decoder_loss: 0.0211 - capsnet_acc: 0.6602 - val_loss: 0.2442 - val_capsnet_loss: 0.2359 - val_decoder_loss: 0.0211 - val_capsnet_acc: 0.6933\n",
      "\n",
      "Epoch 00003: val_capsnet_acc improved from 0.65639 to 0.69328, saving model to ../models/best_weights_capsule_new_train.h5\n",
      "Epoch 4/100\n",
      "7264/7264 [==============================] - 192s 26ms/step - loss: 0.2392 - capsnet_loss: 0.2312 - decoder_loss: 0.0205 - capsnet_acc: 0.6985 - val_loss: 0.2523 - val_capsnet_loss: 0.2441 - val_decoder_loss: 0.0208 - val_capsnet_acc: 0.6465\n",
      "\n",
      "Epoch 00004: val_capsnet_acc did not improve from 0.69328\n",
      "Epoch 5/100\n",
      "7264/7264 [==============================] - 226s 31ms/step - loss: 0.2309 - capsnet_loss: 0.2230 - decoder_loss: 0.0201 - capsnet_acc: 0.7113 - val_loss: 0.2231 - val_capsnet_loss: 0.2151 - val_decoder_loss: 0.0205 - val_capsnet_acc: 0.7181\n",
      "\n",
      "Epoch 00005: val_capsnet_acc improved from 0.69328 to 0.71806, saving model to ../models/best_weights_capsule_new_train.h5\n",
      "Epoch 6/100\n",
      "7264/7264 [==============================] - 203s 28ms/step - loss: 0.2185 - capsnet_loss: 0.2107 - decoder_loss: 0.0199 - capsnet_acc: 0.7358 - val_loss: 0.2303 - val_capsnet_loss: 0.2222 - val_decoder_loss: 0.0205 - val_capsnet_acc: 0.7070\n",
      "\n",
      "Epoch 00006: val_capsnet_acc did not improve from 0.71806\n",
      "Epoch 7/100\n",
      "7264/7264 [==============================] - 198s 27ms/step - loss: 0.2049 - capsnet_loss: 0.1972 - decoder_loss: 0.0196 - capsnet_acc: 0.7470 - val_loss: 0.2188 - val_capsnet_loss: 0.2107 - val_decoder_loss: 0.0205 - val_capsnet_acc: 0.7252\n",
      "\n",
      "Epoch 00007: val_capsnet_acc improved from 0.71806 to 0.72522, saving model to ../models/best_weights_capsule_new_train.h5\n",
      "Epoch 8/100\n",
      "7264/7264 [==============================] - 195s 27ms/step - loss: 0.1940 - capsnet_loss: 0.1864 - decoder_loss: 0.0194 - capsnet_acc: 0.7639 - val_loss: 0.2017 - val_capsnet_loss: 0.1938 - val_decoder_loss: 0.0200 - val_capsnet_acc: 0.7616\n",
      "\n",
      "Epoch 00008: val_capsnet_acc improved from 0.72522 to 0.76156, saving model to ../models/best_weights_capsule_new_train.h5\n",
      "Epoch 9/100\n",
      "7264/7264 [==============================] - 192s 26ms/step - loss: 0.1928 - capsnet_loss: 0.1852 - decoder_loss: 0.0193 - capsnet_acc: 0.7653 - val_loss: 0.1994 - val_capsnet_loss: 0.1916 - val_decoder_loss: 0.0199 - val_capsnet_acc: 0.7566\n",
      "\n",
      "Epoch 00009: val_capsnet_acc did not improve from 0.76156\n",
      "Epoch 10/100\n",
      "7264/7264 [==============================] - 192s 26ms/step - loss: 0.1760 - capsnet_loss: 0.1685 - decoder_loss: 0.0191 - capsnet_acc: 0.7954 - val_loss: 0.2095 - val_capsnet_loss: 0.2017 - val_decoder_loss: 0.0199 - val_capsnet_acc: 0.7357\n",
      "\n",
      "Epoch 00010: val_capsnet_acc did not improve from 0.76156\n",
      "Epoch 11/100\n",
      "7264/7264 [==============================] - 192s 26ms/step - loss: 0.1685 - capsnet_loss: 0.1611 - decoder_loss: 0.0189 - capsnet_acc: 0.8040 - val_loss: 0.1847 - val_capsnet_loss: 0.1769 - val_decoder_loss: 0.0199 - val_capsnet_acc: 0.7803\n",
      "\n",
      "Epoch 00011: val_capsnet_acc improved from 0.76156 to 0.78029, saving model to ../models/best_weights_capsule_new_train.h5\n",
      "Epoch 12/100\n",
      "7264/7264 [==============================] - 192s 26ms/step - loss: 0.1636 - capsnet_loss: 0.1562 - decoder_loss: 0.0189 - capsnet_acc: 0.8137 - val_loss: 0.1813 - val_capsnet_loss: 0.1736 - val_decoder_loss: 0.0196 - val_capsnet_acc: 0.7814\n",
      "\n",
      "Epoch 00012: val_capsnet_acc improved from 0.78029 to 0.78139, saving model to ../models/best_weights_capsule_new_train.h5\n",
      "Epoch 13/100\n",
      "7264/7264 [==============================] - 192s 26ms/step - loss: 0.1542 - capsnet_loss: 0.1469 - decoder_loss: 0.0187 - capsnet_acc: 0.8298 - val_loss: 0.1942 - val_capsnet_loss: 0.1865 - val_decoder_loss: 0.0197 - val_capsnet_acc: 0.7572\n",
      "\n",
      "Epoch 00013: val_capsnet_acc did not improve from 0.78139\n",
      "Epoch 14/100\n",
      "7264/7264 [==============================] - 192s 26ms/step - loss: 0.1517 - capsnet_loss: 0.1444 - decoder_loss: 0.0186 - capsnet_acc: 0.8318 - val_loss: 0.1988 - val_capsnet_loss: 0.1911 - val_decoder_loss: 0.0197 - val_capsnet_acc: 0.7406\n",
      "\n",
      "Epoch 00014: val_capsnet_acc did not improve from 0.78139\n",
      "Epoch 15/100\n",
      "7264/7264 [==============================] - 192s 26ms/step - loss: 0.1439 - capsnet_loss: 0.1367 - decoder_loss: 0.0184 - capsnet_acc: 0.8428 - val_loss: 0.2038 - val_capsnet_loss: 0.1960 - val_decoder_loss: 0.0199 - val_capsnet_acc: 0.7373\n",
      "\n",
      "Epoch 00015: val_capsnet_acc did not improve from 0.78139\n",
      "Epoch 16/100\n",
      "7264/7264 [==============================] - 192s 26ms/step - loss: 0.1374 - capsnet_loss: 0.1303 - decoder_loss: 0.0183 - capsnet_acc: 0.8545 - val_loss: 0.1902 - val_capsnet_loss: 0.1825 - val_decoder_loss: 0.0198 - val_capsnet_acc: 0.7682\n",
      "\n",
      "Epoch 00016: val_capsnet_acc did not improve from 0.78139\n",
      "Epoch 17/100\n",
      "7264/7264 [==============================] - 192s 26ms/step - loss: 0.1302 - capsnet_loss: 0.1230 - decoder_loss: 0.0182 - capsnet_acc: 0.8663 - val_loss: 0.1738 - val_capsnet_loss: 0.1662 - val_decoder_loss: 0.0194 - val_capsnet_acc: 0.8084\n",
      "\n",
      "Epoch 00017: val_capsnet_acc improved from 0.78139 to 0.80837, saving model to ../models/best_weights_capsule_new_train.h5\n",
      "Epoch 18/100\n",
      "7264/7264 [==============================] - 192s 26ms/step - loss: 0.1262 - capsnet_loss: 0.1191 - decoder_loss: 0.0181 - capsnet_acc: 0.8757 - val_loss: 0.1972 - val_capsnet_loss: 0.1895 - val_decoder_loss: 0.0194 - val_capsnet_acc: 0.7748\n",
      "\n",
      "Epoch 00018: val_capsnet_acc did not improve from 0.80837\n",
      "Epoch 19/100\n",
      "7264/7264 [==============================] - 201s 28ms/step - loss: 0.1197 - capsnet_loss: 0.1127 - decoder_loss: 0.0180 - capsnet_acc: 0.8797 - val_loss: 0.1807 - val_capsnet_loss: 0.1730 - val_decoder_loss: 0.0196 - val_capsnet_acc: 0.7852\n",
      "\n",
      "Epoch 00019: val_capsnet_acc did not improve from 0.80837\n",
      "Epoch 20/100\n",
      "7264/7264 [==============================] - 240s 33ms/step - loss: 0.1104 - capsnet_loss: 0.1034 - decoder_loss: 0.0179 - capsnet_acc: 0.8968 - val_loss: 0.1680 - val_capsnet_loss: 0.1604 - val_decoder_loss: 0.0193 - val_capsnet_acc: 0.8029\n",
      "\n",
      "Epoch 00020: val_capsnet_acc did not improve from 0.80837\n",
      "Epoch 21/100\n",
      "7264/7264 [==============================] - 189s 26ms/step - loss: 0.1050 - capsnet_loss: 0.0980 - decoder_loss: 0.0178 - capsnet_acc: 0.9053 - val_loss: 0.2150 - val_capsnet_loss: 0.2072 - val_decoder_loss: 0.0199 - val_capsnet_acc: 0.7302\n",
      "\n",
      "Epoch 00021: val_capsnet_acc did not improve from 0.80837\n",
      "Epoch 22/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7264/7264 [==============================] - 195s 27ms/step - loss: 0.1003 - capsnet_loss: 0.0933 - decoder_loss: 0.0177 - capsnet_acc: 0.9096 - val_loss: 0.1581 - val_capsnet_loss: 0.1506 - val_decoder_loss: 0.0192 - val_capsnet_acc: 0.8238\n",
      "\n",
      "Epoch 00022: val_capsnet_acc improved from 0.80837 to 0.82379, saving model to ../models/best_weights_capsule_new_train.h5\n",
      "Epoch 23/100\n",
      "7264/7264 [==============================] - 198s 27ms/step - loss: 0.0916 - capsnet_loss: 0.0847 - decoder_loss: 0.0177 - capsnet_acc: 0.9228 - val_loss: 0.1741 - val_capsnet_loss: 0.1665 - val_decoder_loss: 0.0195 - val_capsnet_acc: 0.7858\n",
      "\n",
      "Epoch 00023: val_capsnet_acc did not improve from 0.82379\n",
      "Epoch 24/100\n",
      "7264/7264 [==============================] - 188s 26ms/step - loss: 0.0869 - capsnet_loss: 0.0800 - decoder_loss: 0.0176 - capsnet_acc: 0.9332 - val_loss: 0.1825 - val_capsnet_loss: 0.1749 - val_decoder_loss: 0.0193 - val_capsnet_acc: 0.7896\n",
      "\n",
      "Epoch 00024: val_capsnet_acc did not improve from 0.82379\n",
      "Epoch 25/100\n",
      "7264/7264 [==============================] - 200s 28ms/step - loss: 0.0816 - capsnet_loss: 0.0748 - decoder_loss: 0.0175 - capsnet_acc: 0.9372 - val_loss: 0.1834 - val_capsnet_loss: 0.1758 - val_decoder_loss: 0.0194 - val_capsnet_acc: 0.7814\n",
      "\n",
      "Epoch 00025: val_capsnet_acc did not improve from 0.82379\n",
      "Epoch 26/100\n",
      "7264/7264 [==============================] - 215s 30ms/step - loss: 0.0778 - capsnet_loss: 0.0710 - decoder_loss: 0.0175 - capsnet_acc: 0.9401 - val_loss: 0.1760 - val_capsnet_loss: 0.1683 - val_decoder_loss: 0.0195 - val_capsnet_acc: 0.7913\n",
      "\n",
      "Epoch 00026: val_capsnet_acc did not improve from 0.82379\n",
      "Epoch 27/100\n",
      "7264/7264 [==============================] - 193s 27ms/step - loss: 0.0720 - capsnet_loss: 0.0652 - decoder_loss: 0.0174 - capsnet_acc: 0.9499 - val_loss: 0.1709 - val_capsnet_loss: 0.1633 - val_decoder_loss: 0.0194 - val_capsnet_acc: 0.8012\n",
      "\n",
      "Epoch 00027: val_capsnet_acc did not improve from 0.82379\n",
      "Epoch 28/100\n",
      "7264/7264 [==============================] - 197s 27ms/step - loss: 0.0665 - capsnet_loss: 0.0597 - decoder_loss: 0.0173 - capsnet_acc: 0.9548 - val_loss: 0.1855 - val_capsnet_loss: 0.1779 - val_decoder_loss: 0.0195 - val_capsnet_acc: 0.7704\n",
      "\n",
      "Epoch 00028: val_capsnet_acc did not improve from 0.82379\n",
      "Epoch 29/100\n",
      "7264/7264 [==============================] - 202s 28ms/step - loss: 0.0619 - capsnet_loss: 0.0552 - decoder_loss: 0.0172 - capsnet_acc: 0.9597 - val_loss: 0.1686 - val_capsnet_loss: 0.1610 - val_decoder_loss: 0.0193 - val_capsnet_acc: 0.8007\n",
      "\n",
      "Epoch 00029: val_capsnet_acc did not improve from 0.82379\n",
      "Epoch 30/100\n",
      "7264/7264 [==============================] - 203s 28ms/step - loss: 0.0564 - capsnet_loss: 0.0497 - decoder_loss: 0.0172 - capsnet_acc: 0.9690 - val_loss: 0.1805 - val_capsnet_loss: 0.1729 - val_decoder_loss: 0.0194 - val_capsnet_acc: 0.7759\n",
      "\n",
      "Epoch 00030: val_capsnet_acc did not improve from 0.82379\n",
      "Epoch 31/100\n",
      "7264/7264 [==============================] - 201s 28ms/step - loss: 0.0575 - capsnet_loss: 0.0508 - decoder_loss: 0.0171 - capsnet_acc: 0.9652 - val_loss: 0.1796 - val_capsnet_loss: 0.1720 - val_decoder_loss: 0.0195 - val_capsnet_acc: 0.7808\n",
      "\n",
      "Epoch 00031: val_capsnet_acc did not improve from 0.82379\n",
      "Epoch 32/100\n",
      "7264/7264 [==============================] - 202s 28ms/step - loss: 0.0538 - capsnet_loss: 0.0471 - decoder_loss: 0.0171 - capsnet_acc: 0.9676 - val_loss: 0.1858 - val_capsnet_loss: 0.1782 - val_decoder_loss: 0.0194 - val_capsnet_acc: 0.7671\n",
      "\n",
      "Epoch 00032: val_capsnet_acc did not improve from 0.82379\n",
      "Epoch 33/100\n",
      "7264/7264 [==============================] - 189s 26ms/step - loss: 0.0478 - capsnet_loss: 0.0411 - decoder_loss: 0.0170 - capsnet_acc: 0.9743 - val_loss: 0.1753 - val_capsnet_loss: 0.1677 - val_decoder_loss: 0.0195 - val_capsnet_acc: 0.7952\n",
      "\n",
      "Epoch 00033: val_capsnet_acc did not improve from 0.82379\n",
      "Epoch 34/100\n",
      "7264/7264 [==============================] - 189s 26ms/step - loss: 0.0426 - capsnet_loss: 0.0359 - decoder_loss: 0.0169 - capsnet_acc: 0.9795 - val_loss: 0.1959 - val_capsnet_loss: 0.1881 - val_decoder_loss: 0.0199 - val_capsnet_acc: 0.7632\n",
      "\n",
      "Epoch 00034: val_capsnet_acc did not improve from 0.82379\n",
      "Epoch 35/100\n",
      "7264/7264 [==============================] - 189s 26ms/step - loss: 0.0428 - capsnet_loss: 0.0362 - decoder_loss: 0.0169 - capsnet_acc: 0.9791 - val_loss: 0.1841 - val_capsnet_loss: 0.1764 - val_decoder_loss: 0.0196 - val_capsnet_acc: 0.7830\n",
      "\n",
      "Epoch 00035: val_capsnet_acc did not improve from 0.82379\n",
      "Epoch 36/100\n",
      "7264/7264 [==============================] - 189s 26ms/step - loss: 0.0528 - capsnet_loss: 0.0462 - decoder_loss: 0.0169 - capsnet_acc: 0.9668 - val_loss: 0.1801 - val_capsnet_loss: 0.1725 - val_decoder_loss: 0.0194 - val_capsnet_acc: 0.7885\n",
      "\n",
      "Epoch 00036: val_capsnet_acc did not improve from 0.82379\n",
      "Epoch 37/100\n",
      "7264/7264 [==============================] - 189s 26ms/step - loss: 0.0379 - capsnet_loss: 0.0313 - decoder_loss: 0.0167 - capsnet_acc: 0.9822 - val_loss: 0.2108 - val_capsnet_loss: 0.2030 - val_decoder_loss: 0.0197 - val_capsnet_acc: 0.7583\n",
      "\n",
      "Epoch 00037: val_capsnet_acc did not improve from 0.82379\n",
      "Epoch 38/100\n",
      "7264/7264 [==============================] - 189s 26ms/step - loss: 0.0371 - capsnet_loss: 0.0306 - decoder_loss: 0.0167 - capsnet_acc: 0.9851 - val_loss: 0.1876 - val_capsnet_loss: 0.1799 - val_decoder_loss: 0.0197 - val_capsnet_acc: 0.7759\n",
      "\n",
      "Epoch 00038: val_capsnet_acc did not improve from 0.82379\n",
      "Epoch 39/100\n",
      "7264/7264 [==============================] - 189s 26ms/step - loss: 0.0365 - capsnet_loss: 0.0300 - decoder_loss: 0.0166 - capsnet_acc: 0.9840 - val_loss: 0.1935 - val_capsnet_loss: 0.1859 - val_decoder_loss: 0.0195 - val_capsnet_acc: 0.7660\n",
      "\n",
      "Epoch 00039: val_capsnet_acc did not improve from 0.82379\n",
      "Epoch 40/100\n",
      "7264/7264 [==============================] - 189s 26ms/step - loss: 0.0339 - capsnet_loss: 0.0274 - decoder_loss: 0.0165 - capsnet_acc: 0.9842 - val_loss: 0.1932 - val_capsnet_loss: 0.1855 - val_decoder_loss: 0.0196 - val_capsnet_acc: 0.7660\n",
      "\n",
      "Epoch 00040: val_capsnet_acc did not improve from 0.82379\n",
      "Epoch 41/100\n",
      "7264/7264 [==============================] - 189s 26ms/step - loss: 0.0412 - capsnet_loss: 0.0347 - decoder_loss: 0.0166 - capsnet_acc: 0.9800 - val_loss: 0.1884 - val_capsnet_loss: 0.1807 - val_decoder_loss: 0.0195 - val_capsnet_acc: 0.7841\n",
      "\n",
      "Epoch 00041: val_capsnet_acc did not improve from 0.82379\n",
      "Epoch 42/100\n",
      "7264/7264 [==============================] - 189s 26ms/step - loss: 0.0313 - capsnet_loss: 0.0249 - decoder_loss: 0.0164 - capsnet_acc: 0.9908 - val_loss: 0.1814 - val_capsnet_loss: 0.1738 - val_decoder_loss: 0.0196 - val_capsnet_acc: 0.7709\n",
      "\n",
      "Epoch 00042: val_capsnet_acc did not improve from 0.82379\n",
      "Epoch 43/100\n",
      "7264/7264 [==============================] - 189s 26ms/step - loss: 0.0305 - capsnet_loss: 0.0241 - decoder_loss: 0.0163 - capsnet_acc: 0.9871 - val_loss: 0.1877 - val_capsnet_loss: 0.1801 - val_decoder_loss: 0.0196 - val_capsnet_acc: 0.7704\n",
      "\n",
      "Epoch 00043: val_capsnet_acc did not improve from 0.82379\n",
      "Epoch 44/100\n",
      "7264/7264 [==============================] - 189s 26ms/step - loss: 0.0304 - capsnet_loss: 0.0240 - decoder_loss: 0.0163 - capsnet_acc: 0.9887 - val_loss: 0.1960 - val_capsnet_loss: 0.1882 - val_decoder_loss: 0.0199 - val_capsnet_acc: 0.7588\n",
      "\n",
      "Epoch 00044: val_capsnet_acc did not improve from 0.82379\n",
      "Epoch 45/100\n",
      "7264/7264 [==============================] - 189s 26ms/step - loss: 0.0316 - capsnet_loss: 0.0252 - decoder_loss: 0.0163 - capsnet_acc: 0.9877 - val_loss: 0.2029 - val_capsnet_loss: 0.1952 - val_decoder_loss: 0.0197 - val_capsnet_acc: 0.7472\n",
      "\n",
      "Epoch 00045: val_capsnet_acc did not improve from 0.82379\n",
      "Epoch 46/100\n",
      "7264/7264 [==============================] - 189s 26ms/step - loss: 0.0291 - capsnet_loss: 0.0227 - decoder_loss: 0.0161 - capsnet_acc: 0.9871 - val_loss: 0.2528 - val_capsnet_loss: 0.2450 - val_decoder_loss: 0.0198 - val_capsnet_acc: 0.7026\n",
      "\n",
      "Epoch 00046: val_capsnet_acc did not improve from 0.82379\n",
      "Epoch 47/100\n",
      "7264/7264 [==============================] - 189s 26ms/step - loss: 0.0306 - capsnet_loss: 0.0243 - decoder_loss: 0.0161 - capsnet_acc: 0.9888 - val_loss: 0.2033 - val_capsnet_loss: 0.1956 - val_decoder_loss: 0.0196 - val_capsnet_acc: 0.7489\n",
      "\n",
      "Epoch 00047: val_capsnet_acc did not improve from 0.82379\n",
      "Epoch 48/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7264/7264 [==============================] - 189s 26ms/step - loss: 0.0225 - capsnet_loss: 0.0163 - decoder_loss: 0.0159 - capsnet_acc: 0.9938 - val_loss: 0.1990 - val_capsnet_loss: 0.1913 - val_decoder_loss: 0.0198 - val_capsnet_acc: 0.7577\n",
      "\n",
      "Epoch 00048: val_capsnet_acc did not improve from 0.82379\n",
      "Epoch 49/100\n",
      "7264/7264 [==============================] - 192s 26ms/step - loss: 0.0380 - capsnet_loss: 0.0316 - decoder_loss: 0.0161 - capsnet_acc: 0.9791 - val_loss: 0.2114 - val_capsnet_loss: 0.2037 - val_decoder_loss: 0.0196 - val_capsnet_acc: 0.7390\n",
      "\n",
      "Epoch 00049: val_capsnet_acc did not improve from 0.82379\n",
      "Epoch 50/100\n",
      "7264/7264 [==============================] - 191s 26ms/step - loss: 0.0279 - capsnet_loss: 0.0217 - decoder_loss: 0.0159 - capsnet_acc: 0.9877 - val_loss: 0.1999 - val_capsnet_loss: 0.1922 - val_decoder_loss: 0.0197 - val_capsnet_acc: 0.7588\n",
      "\n",
      "Epoch 00050: val_capsnet_acc did not improve from 0.82379\n",
      "Epoch 51/100\n",
      "7264/7264 [==============================] - 191s 26ms/step - loss: 0.0211 - capsnet_loss: 0.0150 - decoder_loss: 0.0157 - capsnet_acc: 0.9946 - val_loss: 0.1930 - val_capsnet_loss: 0.1853 - val_decoder_loss: 0.0195 - val_capsnet_acc: 0.7704\n",
      "\n",
      "Epoch 00051: val_capsnet_acc did not improve from 0.82379\n",
      "Epoch 52/100\n",
      "1824/7264 [======>.......................] - ETA: 2:18 - loss: 0.0265 - capsnet_loss: 0.0205 - decoder_loss: 0.0154 - capsnet_acc: 0.9885"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-87ef41242976>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m checkpoint = ModelCheckpoint(weight_path, monitor='val_capsnet_acc',\n\u001b[1;32m      9\u001b[0m                                            save_best_only=True, save_weights_only=True, verbose=1)\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tensorlow-gpu/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/envs/tensorlow-gpu/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorlow-gpu/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorlow-gpu/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorlow-gpu/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#fitting the model\n",
    "model.compile(optimizer=optimizers.Adam(lr=0.001),\n",
    "                  loss=[margin_loss, 'mse'],\n",
    "                  loss_weights=[1., 0.392],\n",
    "                  metrics={'capsnet': 'accuracy'})\n",
    "\n",
    "weight_path = '../models/best_weights_capsule_new_train.h5'\n",
    "checkpoint = ModelCheckpoint(weight_path, monitor='val_capsnet_acc',\n",
    "                                           save_best_only=True, save_weights_only=True, verbose=1)\n",
    "model.fit([x_train, y_train], [y_train, x_train], validation_data = [[x_test, y_test], [y_test, x_test]], epochs = 100, batch_size = 16, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "base_dir = \"../dataset\"\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale= 1./255, shear_range=0.2, zoom_range=0.2, width_shift_range=0.2, height_shift_range=0.2, fill_mode='nearest')\n",
    "valid_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "training_set = train_datagen.flow_from_directory(base_dir+'/train', target_size=(224,224),batch_size = batch_size, class_mode='categorical')\n",
    "valid_set = valid_datagen.flow_from_directory(base_dir+'/valid', target_size=(224,224),batch_size = batch_size, class_mode='categorical')\n",
    "\n",
    "\n",
    "class_dict = training_set.class_indices\n",
    "li = list(class_dict)\n",
    "\n",
    "training_num = training_set.samples\n",
    "valid_num = valid_set.samples\n",
    "\n",
    "weight_path = '../models/best_weights_capsule_new.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(weight_path, monitor='val_capsnet_acc',\n",
    "                                           save_best_only=True, save_weights_only=False, verbose=1)\n",
    "\n",
    "model.compile(optimizer=optimizers.Adam(lr=0.001),\n",
    "                  loss=[margin_loss, 'mse'],\n",
    "                  loss_weights=[1., 0.392],\n",
    "                  metrics={'capsnet': 'accuracy'})\n",
    "\n",
    "model.fit_generator(training_set, steps_per_epoch=training_num//batch_size, epochs=100,validation_data=valid_set, validation_steps=valid_num//batch_size, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
