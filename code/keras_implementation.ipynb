{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/michael/anaconda3/envs/tensorlow-gpu/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/michael/anaconda3/envs/tensorlow-gpu/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/michael/anaconda3/envs/tensorlow-gpu/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/michael/anaconda3/envs/tensorlow-gpu/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/michael/anaconda3/envs/tensorlow-gpu/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/michael/anaconda3/envs/tensorlow-gpu/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras import layers, models, optimizers\n",
    "from keras import backend as K\n",
    "from keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import combine_images\n",
    "from PIL import Image\n",
    "from capsulelayers import CapsuleLayer, PrimaryCap, Length, Mask\n",
    "\n",
    "#importint the necessary lirbraries\n",
    "\n",
    "#import tensorflow.keras as kerasfrom __future__ import print_function\n",
    "\n",
    "import keras\n",
    "from keras.layers import Convolution2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras import optimizers\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.layers import Layer\n",
    "from keras import activations\n",
    "from keras import utils\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras import optimizers\n",
    "from keras.layers import *\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from os import listdir\n",
    "import os\n",
    "from imutils import paths\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import random\n",
    "\n",
    "K.set_image_data_format('channels_last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CapsNet(input_shape, n_class, routings):\n",
    "    \"\"\"\n",
    "    A Capsule Network on MNIST.\n",
    "    :param input_shape: data shape, 3d, [width, height, channels]\n",
    "    :param n_class: number of classes\n",
    "    :param routings: number of routing iterations\n",
    "    :return: Two Keras Models, the first one used for training, and the second one for evaluation.\n",
    "            `eval_model` can also be used for training.\n",
    "    \"\"\"\n",
    "    x = layers.Input(shape=input_shape)\n",
    "\n",
    "    # Layer 1: Just a conventional Conv2D layer\n",
    "    conv1 = layers.Conv2D(filters=256, kernel_size=9, strides=1, padding='valid', activation='relu', name='conv1')(x)\n",
    "\n",
    "    # Layer 2: Conv2D layer with `squash` activation, then reshape to [None, num_capsule, dim_capsule]\n",
    "    primarycaps = PrimaryCap(conv1, dim_capsule=8, n_channels=32, kernel_size=9, strides=2, padding='valid')\n",
    "\n",
    "    # Layer 3: Capsule layer. Routing algorithm works here.\n",
    "    digitcaps = CapsuleLayer(num_capsule=n_class, dim_capsule=16, routings=routings,\n",
    "                             name='digitcaps')(primarycaps)\n",
    "\n",
    "    # Layer 4: This is an auxiliary layer to replace each capsule with its length. Just to match the true label's shape.\n",
    "    # If using tensorflow, this will not be necessary. :)\n",
    "    out_caps = Length(name='capsnet')(digitcaps)\n",
    "\n",
    "    # Decoder network.\n",
    "    y = layers.Input(shape=(n_class,))\n",
    "    masked_by_y = Mask()([digitcaps, y])  # The true label is used to mask the output of capsule layer. For training\n",
    "    masked = Mask()(digitcaps)  # Mask using the capsule with maximal length. For prediction\n",
    "\n",
    "    # Shared Decoder model in training and prediction\n",
    "    decoder = models.Sequential(name='decoder')\n",
    "    decoder.add(layers.Dense(512, activation='relu', input_dim=16*n_class))\n",
    "    decoder.add(layers.Dense(1024, activation='relu'))\n",
    "    decoder.add(layers.Dense(np.prod(input_shape), activation='sigmoid'))\n",
    "    decoder.add(layers.Reshape(target_shape=input_shape, name='out_recon'))\n",
    "\n",
    "    # Models for training and evaluation (prediction)\n",
    "    train_model = models.Model([x, y], [out_caps, decoder(masked_by_y)])\n",
    "    eval_model = models.Model(x, [out_caps, decoder(masked)])\n",
    "\n",
    "    # manipulate model\n",
    "    noise = layers.Input(shape=(n_class, 16))\n",
    "    noised_digitcaps = layers.Add()([digitcaps, noise])\n",
    "    masked_noised_y = Mask()([noised_digitcaps, y])\n",
    "    manipulate_model = models.Model([x, y, noise], decoder(masked_noised_y))\n",
    "    return train_model, eval_model, manipulate_model\n",
    "\n",
    "\n",
    "def margin_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Margin loss for Eq.(4). When y_true[i, :] contains not just one `1`, this loss should work too. Not test it.\n",
    "    :param y_true: [None, n_classes]\n",
    "    :param y_pred: [None, num_capsule]\n",
    "    :return: a scalar loss value.\n",
    "    \"\"\"\n",
    "    L = y_true * K.square(K.maximum(0., 0.9 - y_pred)) + \\\n",
    "        0.5 * (1 - y_true) * K.square(K.maximum(0., y_pred - 0.1))\n",
    "\n",
    "    return K.mean(K.sum(L, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/michael/anaconda3/envs/tensorlow-gpu/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/michael/anaconda3/envs/tensorlow-gpu/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/michael/anaconda3/envs/tensorlow-gpu/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/michael/Desktop/Datascience Projects/goz-plant-disease-project/project/code/capsulelayers.py:145: calling softmax (from tensorflow.python.ops.nn_ops) with dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "dim is deprecated, use axis instead\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 28, 28, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 20, 20, 256)  20992       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_conv2d (Conv2D)      (None, 6, 6, 256)    5308672     conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_reshape (Reshape)    (None, 1152, 8)      0           primarycap_conv2d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_squash (Lambda)      (None, 1152, 8)      0           primarycap_reshape[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "digitcaps (CapsuleLayer)        (None, 10, 16)       1474560     primarycap_squash[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 10)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "mask_1 (Mask)                   (None, 160)          0           digitcaps[0][0]                  \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "capsnet (Length)                (None, 10)           0           digitcaps[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "decoder (Sequential)            (None, 28, 28, 1)    1411344     mask_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 8,215,568\n",
      "Trainable params: 8,215,568\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#define the model\n",
    "model, eval_model, manipulate_model = CapsNet(input_shape=(28, 28, 1),\n",
    "                                                  n_class=10,\n",
    "                                                  routings=3)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] splitting data\n"
     ]
    }
   ],
   "source": [
    "#Now rewriting the algorithm to preprocess about 500 images in my own pipeline\n",
    "#only do this if you have enough RAM\n",
    "def convert_image_to_array(image_dir):\n",
    "    try:\n",
    "        image = cv2.imread(image_dir, 0)\n",
    "        if image is not None:\n",
    "            image = cv2.resize(image, (28, 28))\n",
    "            return img_to_array(image)\n",
    "        else:\n",
    "            return np.array([])\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error : {e}\")\n",
    "        return None\n",
    "\n",
    "image_list = []\n",
    "\n",
    "directory_root = '../dataset/greyscaled/'\n",
    "imagePaths = list(paths.list_images(directory_root))\n",
    "imagePaths = imagePaths[0:-1:2]\n",
    "random.shuffle(imagePaths)\n",
    "labels = [p.split(os.path.sep)[-2] for p in imagePaths]\n",
    "\n",
    "batch_size = 16\n",
    "try:\n",
    "    for i in np.arange(0, len(imagePaths), batch_size):\n",
    "        batchPaths = imagePaths[i:i + batch_size]\n",
    "        batchLabels = labels[i: i + batch_size]\n",
    "        \n",
    "        for image in batchPaths:\n",
    "            image_list.append(convert_image_to_array(image))\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "\n",
    "len(image_list), len(labels)\n",
    "\n",
    "np_image_list = np.array(image_list, dtype = np.float16) / 255.0\n",
    "print(f\"[INFO] splitting data\")\n",
    "x_train, x_test, y_train, y_test = train_test_split(np_image_list, labels, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9080, 28, 28, 1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_image_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = LabelBinarizer().fit_transform(y_test)\n",
    "y_train = LabelBinarizer().fit_transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7264, 28, 28, 1), (1816, 28, 28, 1))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = np.array(x_train)\n",
    "x_test = np.array(x_test)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "x_train.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7264, 28, 28, 1), (1816, 28, 28, 1))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = x_train.reshape((x_train.shape[0], 28, 28, 1))\n",
    "x_test = x_test.reshape((x_test.shape[0], 28, 28, 1))\n",
    "x_train.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1816, 10)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape, len(y_test)\n",
    "classNames = [str(x) for x in np.unique(labels)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7264 samples, validate on 1816 samples\n",
      "Epoch 1/100\n",
      "7264/7264 [==============================] - 240s 33ms/step - loss: 0.7131 - capsnet_loss: 0.7039 - decoder_loss: 0.0234 - capsnet_acc: 0.2338 - val_loss: 0.5576 - val_capsnet_loss: 0.5488 - val_decoder_loss: 0.0224 - val_capsnet_acc: 0.2996\n",
      "\n",
      "Epoch 00001: val_capsnet_acc improved from -inf to 0.29956, saving model to ../models/best_weights_capsule_new_greyscale.h5\n",
      "Epoch 2/100\n",
      "7264/7264 [==============================] - 236s 32ms/step - loss: 0.5189 - capsnet_loss: 0.5103 - decoder_loss: 0.0219 - capsnet_acc: 0.2748 - val_loss: 0.5176 - val_capsnet_loss: 0.5093 - val_decoder_loss: 0.0212 - val_capsnet_acc: 0.1806\n",
      "\n",
      "Epoch 00002: val_capsnet_acc did not improve from 0.29956\n",
      "Epoch 3/100\n",
      "7264/7264 [==============================] - 240s 33ms/step - loss: 0.4710 - capsnet_loss: 0.4628 - decoder_loss: 0.0211 - capsnet_acc: 0.3248 - val_loss: 0.4315 - val_capsnet_loss: 0.4234 - val_decoder_loss: 0.0209 - val_capsnet_acc: 0.3667\n",
      "\n",
      "Epoch 00003: val_capsnet_acc improved from 0.29956 to 0.36674, saving model to ../models/best_weights_capsule_new_greyscale.h5\n",
      "Epoch 4/100\n",
      "7264/7264 [==============================] - 243s 33ms/step - loss: 0.4240 - capsnet_loss: 0.4164 - decoder_loss: 0.0196 - capsnet_acc: 0.4108 - val_loss: 0.4142 - val_capsnet_loss: 0.4066 - val_decoder_loss: 0.0193 - val_capsnet_acc: 0.4091\n",
      "\n",
      "Epoch 00004: val_capsnet_acc improved from 0.36674 to 0.40914, saving model to ../models/best_weights_capsule_new_greyscale.h5\n",
      "Epoch 5/100\n",
      "7264/7264 [==============================] - 240s 33ms/step - loss: 0.3860 - capsnet_loss: 0.3787 - decoder_loss: 0.0187 - capsnet_acc: 0.4778 - val_loss: 0.3735 - val_capsnet_loss: 0.3661 - val_decoder_loss: 0.0188 - val_capsnet_acc: 0.4994\n",
      "\n",
      "Epoch 00005: val_capsnet_acc improved from 0.40914 to 0.49945, saving model to ../models/best_weights_capsule_new_greyscale.h5\n",
      "Epoch 6/100\n",
      "7264/7264 [==============================] - 240s 33ms/step - loss: 0.3578 - capsnet_loss: 0.3507 - decoder_loss: 0.0182 - capsnet_acc: 0.5245 - val_loss: 0.3501 - val_capsnet_loss: 0.3428 - val_decoder_loss: 0.0186 - val_capsnet_acc: 0.5303\n",
      "\n",
      "Epoch 00006: val_capsnet_acc improved from 0.49945 to 0.53029, saving model to ../models/best_weights_capsule_new_greyscale.h5\n",
      "Epoch 7/100\n",
      "7264/7264 [==============================] - 237s 33ms/step - loss: 0.3377 - capsnet_loss: 0.3308 - decoder_loss: 0.0177 - capsnet_acc: 0.5527 - val_loss: 0.3463 - val_capsnet_loss: 0.3394 - val_decoder_loss: 0.0176 - val_capsnet_acc: 0.5562\n",
      "\n",
      "Epoch 00007: val_capsnet_acc improved from 0.53029 to 0.55617, saving model to ../models/best_weights_capsule_new_greyscale.h5\n",
      "Epoch 8/100\n",
      "7264/7264 [==============================] - 239s 33ms/step - loss: 0.3178 - capsnet_loss: 0.3111 - decoder_loss: 0.0172 - capsnet_acc: 0.5869 - val_loss: 0.3377 - val_capsnet_loss: 0.3309 - val_decoder_loss: 0.0174 - val_capsnet_acc: 0.5694\n",
      "\n",
      "Epoch 00008: val_capsnet_acc improved from 0.55617 to 0.56938, saving model to ../models/best_weights_capsule_new_greyscale.h5\n",
      "Epoch 9/100\n",
      "7264/7264 [==============================] - 237s 33ms/step - loss: 0.3018 - capsnet_loss: 0.2953 - decoder_loss: 0.0166 - capsnet_acc: 0.6099 - val_loss: 0.3192 - val_capsnet_loss: 0.3126 - val_decoder_loss: 0.0168 - val_capsnet_acc: 0.5980\n",
      "\n",
      "Epoch 00009: val_capsnet_acc improved from 0.56938 to 0.59802, saving model to ../models/best_weights_capsule_new_greyscale.h5\n",
      "Epoch 10/100\n",
      "7264/7264 [==============================] - 237s 33ms/step - loss: 0.2843 - capsnet_loss: 0.2779 - decoder_loss: 0.0163 - capsnet_acc: 0.6356 - val_loss: 0.3159 - val_capsnet_loss: 0.3093 - val_decoder_loss: 0.0168 - val_capsnet_acc: 0.5865\n",
      "\n",
      "Epoch 00010: val_capsnet_acc did not improve from 0.59802\n",
      "Epoch 11/100\n",
      "7264/7264 [==============================] - 237s 33ms/step - loss: 0.2688 - capsnet_loss: 0.2625 - decoder_loss: 0.0160 - capsnet_acc: 0.6653 - val_loss: 0.3147 - val_capsnet_loss: 0.3083 - val_decoder_loss: 0.0165 - val_capsnet_acc: 0.5865\n",
      "\n",
      "Epoch 00011: val_capsnet_acc did not improve from 0.59802\n",
      "Epoch 12/100\n",
      "7264/7264 [==============================] - 237s 33ms/step - loss: 0.2509 - capsnet_loss: 0.2447 - decoder_loss: 0.0157 - capsnet_acc: 0.6975 - val_loss: 0.3250 - val_capsnet_loss: 0.3183 - val_decoder_loss: 0.0171 - val_capsnet_acc: 0.6013\n",
      "\n",
      "Epoch 00012: val_capsnet_acc improved from 0.59802 to 0.60132, saving model to ../models/best_weights_capsule_new_greyscale.h5\n",
      "Epoch 13/100\n",
      "7264/7264 [==============================] - 237s 33ms/step - loss: 0.2373 - capsnet_loss: 0.2313 - decoder_loss: 0.0154 - capsnet_acc: 0.7226 - val_loss: 0.3081 - val_capsnet_loss: 0.3018 - val_decoder_loss: 0.0162 - val_capsnet_acc: 0.6030\n",
      "\n",
      "Epoch 00013: val_capsnet_acc improved from 0.60132 to 0.60297, saving model to ../models/best_weights_capsule_new_greyscale.h5\n",
      "Epoch 14/100\n",
      "7264/7264 [==============================] - 237s 33ms/step - loss: 0.2170 - capsnet_loss: 0.2111 - decoder_loss: 0.0152 - capsnet_acc: 0.7596 - val_loss: 0.3011 - val_capsnet_loss: 0.2947 - val_decoder_loss: 0.0164 - val_capsnet_acc: 0.6145\n",
      "\n",
      "Epoch 00014: val_capsnet_acc improved from 0.60297 to 0.61454, saving model to ../models/best_weights_capsule_new_greyscale.h5\n",
      "Epoch 15/100\n",
      "7264/7264 [==============================] - 239s 33ms/step - loss: 0.2003 - capsnet_loss: 0.1944 - decoder_loss: 0.0149 - capsnet_acc: 0.7936 - val_loss: 0.2881 - val_capsnet_loss: 0.2818 - val_decoder_loss: 0.0160 - val_capsnet_acc: 0.6344\n",
      "\n",
      "Epoch 00015: val_capsnet_acc improved from 0.61454 to 0.63436, saving model to ../models/best_weights_capsule_new_greyscale.h5\n",
      "Epoch 16/100\n",
      "7264/7264 [==============================] - 238s 33ms/step - loss: 0.1842 - capsnet_loss: 0.1785 - decoder_loss: 0.0147 - capsnet_acc: 0.8237 - val_loss: 0.2912 - val_capsnet_loss: 0.2849 - val_decoder_loss: 0.0160 - val_capsnet_acc: 0.6162\n",
      "\n",
      "Epoch 00016: val_capsnet_acc did not improve from 0.63436\n",
      "Epoch 17/100\n",
      "7264/7264 [==============================] - 238s 33ms/step - loss: 0.1663 - capsnet_loss: 0.1606 - decoder_loss: 0.0145 - capsnet_acc: 0.8590 - val_loss: 0.2909 - val_capsnet_loss: 0.2845 - val_decoder_loss: 0.0163 - val_capsnet_acc: 0.6333\n",
      "\n",
      "Epoch 00017: val_capsnet_acc did not improve from 0.63436\n",
      "Epoch 18/100\n",
      "7264/7264 [==============================] - 238s 33ms/step - loss: 0.1487 - capsnet_loss: 0.1431 - decoder_loss: 0.0143 - capsnet_acc: 0.8882 - val_loss: 0.2835 - val_capsnet_loss: 0.2772 - val_decoder_loss: 0.0160 - val_capsnet_acc: 0.6432\n",
      "\n",
      "Epoch 00018: val_capsnet_acc improved from 0.63436 to 0.64317, saving model to ../models/best_weights_capsule_new_greyscale.h5\n",
      "Epoch 19/100\n",
      "7264/7264 [==============================] - 238s 33ms/step - loss: 0.1350 - capsnet_loss: 0.1295 - decoder_loss: 0.0141 - capsnet_acc: 0.9097 - val_loss: 0.2924 - val_capsnet_loss: 0.2861 - val_decoder_loss: 0.0160 - val_capsnet_acc: 0.6289\n",
      "\n",
      "Epoch 00019: val_capsnet_acc did not improve from 0.64317\n",
      "Epoch 20/100\n",
      "7264/7264 [==============================] - 239s 33ms/step - loss: 0.1174 - capsnet_loss: 0.1120 - decoder_loss: 0.0139 - capsnet_acc: 0.9305 - val_loss: 0.2861 - val_capsnet_loss: 0.2798 - val_decoder_loss: 0.0160 - val_capsnet_acc: 0.6316\n",
      "\n",
      "Epoch 00020: val_capsnet_acc did not improve from 0.64317\n",
      "Epoch 21/100\n",
      "7264/7264 [==============================] - 240s 33ms/step - loss: 0.1031 - capsnet_loss: 0.0977 - decoder_loss: 0.0138 - capsnet_acc: 0.9470 - val_loss: 0.2809 - val_capsnet_loss: 0.2745 - val_decoder_loss: 0.0161 - val_capsnet_acc: 0.6470\n",
      "\n",
      "Epoch 00021: val_capsnet_acc improved from 0.64317 to 0.64703, saving model to ../models/best_weights_capsule_new_greyscale.h5\n",
      "Epoch 22/100\n",
      "7264/7264 [==============================] - 240s 33ms/step - loss: 0.0878 - capsnet_loss: 0.0824 - decoder_loss: 0.0135 - capsnet_acc: 0.9597 - val_loss: 0.2865 - val_capsnet_loss: 0.2801 - val_decoder_loss: 0.0163 - val_capsnet_acc: 0.6459\n",
      "\n",
      "Epoch 00022: val_capsnet_acc did not improve from 0.64703\n",
      "Epoch 23/100\n",
      "7264/7264 [==============================] - 240s 33ms/step - loss: 0.0781 - capsnet_loss: 0.0728 - decoder_loss: 0.0134 - capsnet_acc: 0.9693 - val_loss: 0.2916 - val_capsnet_loss: 0.2853 - val_decoder_loss: 0.0161 - val_capsnet_acc: 0.6421\n",
      "\n",
      "Epoch 00023: val_capsnet_acc did not improve from 0.64703\n",
      "Epoch 24/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7264/7264 [==============================] - 240s 33ms/step - loss: 0.0656 - capsnet_loss: 0.0604 - decoder_loss: 0.0133 - capsnet_acc: 0.9792 - val_loss: 0.2771 - val_capsnet_loss: 0.2708 - val_decoder_loss: 0.0161 - val_capsnet_acc: 0.6542\n",
      "\n",
      "Epoch 00024: val_capsnet_acc improved from 0.64703 to 0.65419, saving model to ../models/best_weights_capsule_new_greyscale.h5\n",
      "Epoch 25/100\n",
      "7264/7264 [==============================] - 250s 34ms/step - loss: 0.0576 - capsnet_loss: 0.0525 - decoder_loss: 0.0131 - capsnet_acc: 0.9849 - val_loss: 0.2908 - val_capsnet_loss: 0.2844 - val_decoder_loss: 0.0164 - val_capsnet_acc: 0.6393\n",
      "\n",
      "Epoch 00025: val_capsnet_acc did not improve from 0.65419\n",
      "Epoch 26/100\n",
      "7264/7264 [==============================] - 251s 35ms/step - loss: 0.0493 - capsnet_loss: 0.0442 - decoder_loss: 0.0129 - capsnet_acc: 0.9877 - val_loss: 0.2844 - val_capsnet_loss: 0.2780 - val_decoder_loss: 0.0162 - val_capsnet_acc: 0.6388\n",
      "\n",
      "Epoch 00026: val_capsnet_acc did not improve from 0.65419\n",
      "Epoch 27/100\n",
      "7264/7264 [==============================] - 241s 33ms/step - loss: 0.0402 - capsnet_loss: 0.0352 - decoder_loss: 0.0127 - capsnet_acc: 0.9927 - val_loss: 0.2893 - val_capsnet_loss: 0.2827 - val_decoder_loss: 0.0167 - val_capsnet_acc: 0.6316\n",
      "\n",
      "Epoch 00027: val_capsnet_acc did not improve from 0.65419\n",
      "Epoch 28/100\n",
      " 320/7264 [>.............................] - ETA: 3:37 - loss: 0.0348 - capsnet_loss: 0.0299 - decoder_loss: 0.0123 - capsnet_acc: 0.9938"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-6febe549e5d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m checkpoint = ModelCheckpoint(weight_path, monitor='val_capsnet_acc',\n\u001b[1;32m      9\u001b[0m                                            save_best_only=True, save_weights_only=True, verbose=1)\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tensorlow-gpu/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/envs/tensorlow-gpu/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorlow-gpu/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorlow-gpu/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorlow-gpu/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#fitting the model\n",
    "model.compile(optimizer=optimizers.Adam(lr=0.001),\n",
    "                  loss=[margin_loss, 'mse'],\n",
    "                  loss_weights=[1., 0.392],\n",
    "                  metrics={'capsnet': 'accuracy'})\n",
    "\n",
    "weight_path = '../models/best_weights_capsule_new_greyscale.h5'\n",
    "checkpoint = ModelCheckpoint(weight_path, monitor='val_capsnet_acc',\n",
    "                                           save_best_only=True, save_weights_only=True, verbose=1)\n",
    "model.fit([x_train, y_train], [y_train, x_train], validation_data = [[x_test, y_test], [y_test, x_test]], epochs = 100, batch_size = 16, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "base_dir = \"../dataset\"\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale= 1./255, shear_range=0.2, zoom_range=0.2, width_shift_range=0.2, height_shift_range=0.2, fill_mode='nearest')\n",
    "valid_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "training_set = train_datagen.flow_from_directory(base_dir+'/train', target_size=(224,224),batch_size = batch_size, class_mode='categorical')\n",
    "valid_set = valid_datagen.flow_from_directory(base_dir+'/valid', target_size=(224,224),batch_size = batch_size, class_mode='categorical')\n",
    "\n",
    "\n",
    "class_dict = training_set.class_indices\n",
    "li = list(class_dict)\n",
    "\n",
    "training_num = training_set.samples\n",
    "valid_num = valid_set.samples\n",
    "\n",
    "weight_path = '../models/best_weights_capsule_new.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(weight_path, monitor='val_capsnet_acc',\n",
    "                                           save_best_only=True, save_weights_only=False, verbose=1)\n",
    "\n",
    "model.compile(optimizer=optimizers.Adam(lr=0.001),\n",
    "                  loss=[margin_loss, 'mse'],\n",
    "                  loss_weights=[1., 0.392],\n",
    "                  metrics={'capsnet': 'accuracy'})\n",
    "\n",
    "model.fit_generator(training_set, steps_per_epoch=training_num//batch_size, epochs=100,validation_data=valid_set, validation_steps=valid_num//batch_size, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
