{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the necessary libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Convolution2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#models that we are bringing in\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__ #It is advised you use tf.2.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting up the preocesses and the classifier object\n",
    "#building the architecture\n",
    "#lets start with AlexNet\n",
    "classifier = Sequential()\n",
    "\n",
    "\n",
    "'''\n",
    "    template of the networks \n",
    "    classifier.add(Convolution2D(, , strides = (), padding = \"\", activation = \"\"))\n",
    "    classifier.add(MaxPooling2D(pool_size = (), strides = (), padding = ))\n",
    "    classifier.add(BatchNormalization())\n",
    "'''\n",
    "\n",
    "\n",
    "#first convolution set\n",
    "classifier.add(Convolution2D(96, 11, strides = (4,4), padding = 'valid', input_shape = (224,224,3), activation = 'relu'))\n",
    "classifier.add(MaxPooling2D(pool_size = (2,2), strides = (2,2), padding = \"valid\"))\n",
    "classifier.add(BatchNormalization())\n",
    "\n",
    "#second convolution set\n",
    "classifier.add(Convolution2D(256, 11, strides = (1,1), padding = \"valid\", activation = \"relu\"))\n",
    "classifier.add(MaxPooling2D(pool_size = (2,2), strides = (2,2), padding = \"valid\"))\n",
    "classifier.add(BatchNormalization())\n",
    "\n",
    "#third convolution set\n",
    "classifier.add(Convolution2D(384, 3, strides = (1,1), padding = \"valid\", activation = \"relu\"))\n",
    "classifier.add(BatchNormalization())\n",
    "\n",
    "#fourth convolution set\n",
    "classifier.add(Convolution2D(384, 3, strides = (1,1), padding = \"valid\", activation = \"relu\"))\n",
    "classifier.add(BatchNormalization())\n",
    "\n",
    "#fifth convolution set\n",
    "classifier.add(Convolution2D(256, 3, strides = (1,1), padding = \"valid\", activation = \"relu\"))\n",
    "classifier.add(MaxPooling2D(pool_size = (2,2), strides = (2,2), padding = \"valid\"))\n",
    "classifier.add(BatchNormalization())\n",
    "\n",
    "#flattening step\n",
    "classifier.add(Flatten())\n",
    "\n",
    "#dense network\n",
    "classifier.add(Dense(units = 4096, activation = \"relu\"))\n",
    "classifier.add(Dropout(0.4))\n",
    "classifier.add(BatchNormalization())\n",
    "\n",
    "classifier.add(Dense(units = 4096, activation = \"relu\"))\n",
    "classifier.add(Dropout(0.4))\n",
    "classifier.add(BatchNormalization())\n",
    "\n",
    "classifier.add(Dense(units = 1000, activation = 'relu'))\n",
    "classifier.add(Dropout(0.2))\n",
    "classifier.add(BatchNormalization())\n",
    "\n",
    "classifier.add(Dense(units = 10, activation = \"softmax\"))\n",
    "classifier.load_weights('../models/best_weights.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mosaic_virus'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "li = ['Bacterial_spot','Early_blight','Late_blight','Leaf_Mold','Septoria_leaf_spot','Target_Spot','Yellow_Leaf_Curl','healthy','mosaic_virus','spider_mite']\n",
    "\n",
    "def get_class(image_path):\n",
    "    img_width, img_height = 224, 224\n",
    "    img = image.load_img(image_path, target_size = (img_width, img_height))\n",
    "    img = image.img_to_array(img)\n",
    "    img = np.expand_dims(img, axis = 0)\n",
    "    value = classifier.predict(img)\n",
    "    value_argmax = np.argmax(value)\n",
    "    return li[value_argmax]\n",
    "\n",
    "get_class('../dataset/train/Yellow_Leaf_Curl/0a5431ac-44ff-4507-a8aa-6eee46235015___UF.GRC_YLCV_Lab 03031.JPG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
